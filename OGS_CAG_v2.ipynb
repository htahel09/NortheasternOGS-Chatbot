{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c430db9-35f8-45f0-a1e0-bcb001060a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import torch\n",
    "from transformers import AutoTokenizer, BitsAndBytesConfig, AutoModelForCausalLM, pipeline\n",
    "from transformers.cache_utils import DynamicCache\n",
    "import time\n",
    "import json \n",
    "import gc\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity \n",
    "import random\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "#from bert_score import BERTScorer\n",
    "import evaluate\n",
    "score = evaluate.load(\"bertscore\", config=\"roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "944d4ef1-1796-4347-acb6-9aaa6ea27fcb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Configure 4-bit quantization to speed up inference and reduce memory usage.\\nquant_config = BitsAndBytesConfig(\\n    load_in_4bit=True,\\n    bnb_4bit_use_double_quant=True,\\n    bnb_4bit_quant_type=\"nf4\",\\n    bnb_4bit_compute_dtype=torch.bfloat16\\n)\\n\\n# Load the tokenizer and model with the optimized quantization configuration.\\nmodel_name = \"microsoft/Phi-3.5-mini-instruct\"\\ntokenizer = AutoTokenizer.from_pretrained(model_name) #, token=access_token)\\nmodel = AutoModelForCausalLM.from_pretrained(\\n    model_name,\\n    quantization_config=quant_config,\\n    device_map=\\'auto\\'\\n)\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# 4-bit quantization to speed up inference and reduce memory\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "# tokenizer and model\n",
    "model_name = \"microsoft/Phi-3.5-mini-instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name) #, token=access_token)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=quant_config,\n",
    "    device_map='auto'\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cafd70d4-13e1-4427-a737-7fbf47efa251",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'content = []\\nwith open(\"json_input_data.json\", \\'r\\', encoding=\"utf-8\") as file:\\n    data = json.load(file)\\n\\nfor page in data:\\n    for sec in page.get(\\'sections\\', []):  # Avoids errors if \\'sections\\' key is missing\\n        content.append(sec.get(\"section_content\", \"\").replace(\"\\r\\n\", \" \").replace(\"\\n\", \" \").strip())  # Strip whitespace\\n\\nprint(f\"Total sections collected: {len(content)}\")\\n\\n# Write to file\\nwith open(\\'all_document.txt\\', \\'w\\', encoding=\"utf-8\") as f:\\n    for i, line in enumerate(content, 1):\\n        f.write(\"%s\\n\" % line)\\n        if line.strip() == \"\":\\n            print(f\"Warning: Empty content at line {i}\")  # Debug empty lines\\n\\n# Count actual written lines\\nwith open(\\'all_document.txt\\', \\'r\\', encoding=\"utf-8\") as f:\\n    written_lines = sum(1 for _ in f)\\n\\nprint(f\"Lines written in all_document.txt: {written_lines}\")'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"content = []\n",
    "with open(\"json_input_data.json\", 'r', encoding=\"utf-8\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "for page in data:\n",
    "    for sec in page.get('sections', []):  # Avoids errors if 'sections' key is missing\n",
    "        content.append(sec.get(\"section_content\", \"\").replace(\"\\r\\n\", \" \").replace(\"\\n\", \" \").strip())  # Strip whitespace\n",
    "\n",
    "print(f\"Total sections collected: {len(content)}\")\n",
    "\n",
    "# Write to file\n",
    "with open('all_document.txt', 'w', encoding=\"utf-8\") as f:\n",
    "    for i, line in enumerate(content, 1):\n",
    "        f.write(\"%s\\n\" % line)\n",
    "        if line.strip() == \"\":\n",
    "            print(f\"Warning: Empty content at line {i}\")  # Debug empty lines\n",
    "\n",
    "# Count actual written lines\n",
    "with open('all_document.txt', 'r', encoding=\"utf-8\") as f:\n",
    "    written_lines = sum(1 for _ in f)\n",
    "\n",
    "print(f\"Lines written in all_document.txt: {written_lines}\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f60466a0-fccd-4291-a003-324ad731a75e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'section_text = []\\nwith open(\"all_document.txt\", \"r\", encoding=\"utf-8\") as file:\\n    section_text = file.read().split(\\'\\n\\')\\nfor x, y in zip(section_text,content):\\n    if x!=y:\\n        print(x)\\n        print(y)\\n        break'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"section_text = []\n",
    "with open(\"all_document.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    section_text = file.read().split('\\n')\n",
    "for x, y in zip(section_text,content):\n",
    "    if x!=y:\n",
    "        print(x)\n",
    "        print(y)\n",
    "        break\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df51db03-1e6c-4387-9deb-a83c32a33111",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Read the context document used for knowledge.\\nwith open(\"document.txt\", \"r\", encoding=\"utf-8\") as file:\\n    document_text = file.read()'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "with open(\"document.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    document_text = file.read()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ce94d008-75e7-4ea4-bb5e-276afe940cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_system_prompt(doc_text, instruction=None):  \n",
    "    # delimiters and headers for PHI.\n",
    "    prompt = f\"\"\"\n",
    "    <|system|>\n",
    "    You are a concise assistant. Provide a single, clear answer to the User Query below using only the provided OGS Website Content. \n",
    "    Do not include extra commentary, repeated phrases, or any information beyond what is in the OGS Website Content. \n",
    "    **If the answer is not available in the OGS Website Content, reply with \"Information not available.\" **\n",
    "    Do not include this language if the information is available. \n",
    "    When your answer is complete, immediately append <|endoftext|> with no additional text.\n",
    "    OGS Website Content:\n",
    "    {\"\\n\".join(doc_text)}\n",
    "    <|end|>\n",
    "    <|user|>\n",
    "    \"\"\".strip()\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25951b65-829e-4e84-9796-2c51630bb621",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_kv_cache(prompt):\n",
    "    \"\"\"\n",
    "    generates a key-value cache  \n",
    "    \"\"\"\n",
    "    # device assignment\n",
    "    device = model.model.embed_tokens.weight.device\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # initialize dynamic cache to store key-value pairs.\n",
    "    kv_cache = DynamicCache()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(\n",
    "            input_ids=input_ids,\n",
    "            past_key_values=kv_cache,\n",
    "            use_cache=True,\n",
    "            output_attentions=False,\n",
    "            output_hidden_states=False\n",
    "        )\n",
    "    # get the sequence length of the cached keys.\n",
    "    cache_length = outputs.past_key_values.key_cache[0].shape[-2]\n",
    "    return outputs.past_key_values, cache_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20cdf04b-0609-46d2-9b5a-78e560d26dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trim_kv_cache(cache, target_length):\n",
    "    \"\"\"\n",
    "    trims kv cache so that only the original doc sequence remains.\n",
    "    \"\"\"\n",
    "    for idx in range(len(cache.key_cache)):\n",
    "        cache.key_cache[idx] = cache.key_cache[idx][:, :, :target_length, :]\n",
    "        cache.value_cache[idx] = cache.value_cache[idx][:, :, :target_length, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "39d6cbb6-312c-4feb-b053-94de3ac85416",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(input_ids, kv_cache, max_tokens=200):\n",
    "    \"\"\"\n",
    "    greedy decoding with the provided KV cache to generate a response.\n",
    "    \"\"\"\n",
    "    # device assignment\n",
    "    device = model.model.embed_tokens.weight.device\n",
    "    input_ids = input_ids.to(device)\n",
    "    generated_tokens = input_ids.clone()\n",
    "    current_token = input_ids\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_tokens):\n",
    "            outputs = model(\n",
    "                input_ids=current_token,\n",
    "                past_key_values=kv_cache,\n",
    "                use_cache=True\n",
    "            )\n",
    "            # get logits for the last token and select the most probable next token.\n",
    "            next_logits = outputs.logits[:, -1, :]\n",
    "            current_token = torch.argmax(next_logits, dim=-1, keepdim=True)\n",
    "            # update cache for the next iteration.\n",
    "            kv_cache = outputs.past_key_values\n",
    "            # append new token to the generated sequence.\n",
    "            generated_tokens = torch.cat([generated_tokens, current_token], dim=1)\n",
    "            # if end-of-sequence token, stop generation.\n",
    "            if model.config.eos_token_id is not None and current_token.item() == model.config.eos_token_id:\n",
    "                break\n",
    "\n",
    "    # Return only the tokens generated after the initial query.\n",
    "    return generated_tokens[:, input_ids.shape[-1]:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e92a47e-9a8b-40b0-be14-507f501d92ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#counts = [len(doc) for doc in content]\n",
    "#sum(counts)/len(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "58d03d63-26f5-4ea0-a08b-d1e1ed6ec9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(len(content))\n",
    "#len(content[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "39e4cc0c-2d9b-4ab3-a036-ad82640f5984",
   "metadata": {},
   "outputs": [],
   "source": [
    "#document_text = content[:20]\n",
    "# Build the system prompt and create the knowledge cache.\n",
    "#system_prompt = prepare_system_prompt(document_text)\n",
    "#knowledge_cache, orig_cache_len = build_kv_cache(system_prompt)\n",
    "#print(\"Initial KV cache length:\", orig_cache_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "63c89f89-4367-4e94-80fb-1f3594dfe74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_response(question, tokenizer, model, knowledge_cache, orig_cache_len):\n",
    "    # append end of prompt tokens and encode query\n",
    "    query = question + \"<|end|>\\n<|assistant|>\\n\" \n",
    "    query_ids = tokenizer.encode(query, return_tensors=\"pt\").to(model.device)\n",
    "    # call generate response and decode\n",
    "    response_ids = generate_response(query_ids, knowledge_cache)\n",
    "    response_text = tokenizer.decode(response_ids[0], skip_special_tokens=True)\n",
    "    # trim cache back to original length\n",
    "    trim_kv_cache(knowledge_cache, orig_cache_len) \n",
    "    return response_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "af543099-c362-4e42-b49d-190b9a82458a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#question = 'Does Northeastern have coop opportunities in the healthcare sector?'\n",
    "#generated_text = query_response(question, tokenizer, model, knowledge_cache, orig_cache_len)\n",
    "#print(f\"Response of the model:\\n {generated_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a28127b0-db86-488d-bfb4-d9b0fdce4503",
   "metadata": {},
   "outputs": [],
   "source": [
    "#end = time.time()\n",
    "#length = end - start \n",
    "# Show the results : this can be altered however you like\n",
    "#print(\"It took\", length, \"seconds!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b90c837c-8387-4ab1-8c1b-0ec876c53187",
   "metadata": {},
   "outputs": [],
   "source": [
    "#question = 'Does Northeastern have coop opportunities in the tech sector?'\n",
    "#generated_text = query_response(question, tokenizer, model, knowledge_cache, orig_cache_len)\n",
    "#print(f\"Response of the model:\\n {generated_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3f4408a9-abe7-482a-92b6-9c3c378fe707",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('all_test_data.txt', \"r\") as file:\n",
    "    all_test_data = json.loads(file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cf83335d-0449-4084-87ae-4303be18737b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_random_n(items, n): \n",
    "    \"\"\"\n",
    "    select n random docs and their associated test QAs from items dict\n",
    "    \"\"\"\n",
    "    if len(items) < n:\n",
    "        # return a copy of the original list if there are fewer than 20 items\n",
    "        return items.copy()\n",
    "    # Randomly sample 20 unique items from the list\n",
    "    randos = random.sample(items, n)\n",
    "    docs = [rand['document'] for rand in randos]\n",
    "    tests = [rand['tests'] for rand in randos]\n",
    "    return docs, tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f874da35-5395-4cc6-bd6e-1c51a76d2e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "#docs20, tests20 = select_random_n(all_test_data,15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "21abb6d7-6b43-4933-a64f-7e0cb90c2f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#len(\"\\n\".join(docs20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ee07b83a-c9b2-42ae-b2a0-7086cb05a990",
   "metadata": {},
   "outputs": [],
   "source": [
    "#docs20[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ced0dd5c-31b3-4392-8972-96642ca2d723",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_cuda_memory():\n",
    "    \"\"\"\n",
    "    manual memory management to avoid memory errors with repeated model use\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # sync/wait for all GPU operations to finish\n",
    "        torch.cuda.synchronize()\n",
    "    except Exception as e:\n",
    "        print(\"Error during synchronization:\", e)\n",
    "    \n",
    "    # garbage collection\n",
    "    gc.collect()\n",
    "    time.sleep(1)  # small delay\n",
    "    \n",
    "    # clears the CUDA cache and reset\n",
    "    try:\n",
    "        torch.cuda.empty_cache()\n",
    "    except Exception as e:\n",
    "        print(\"Error during empty_cache:\", e) \n",
    "    try:\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "    except Exception as e:\n",
    "        print(\"Error during reset_peak_memory_stats:\", e)\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7bb66562-cb1d-49e3-8836-0e7f1b3b0956",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_generated_answer(generated_answer, true_answer, threshold=0.7, max_retries=5):\n",
    "    \"\"\"\n",
    "    evaluates whether a generated answer matches a true answer using multiple modern metrics:\n",
    "      - Token-level F1 score: measures word-overlap after normalization.\n",
    "      - Embedding similarity: cosine similarity between mean-pooled embeddings.\n",
    "      - Zero-shot NLI: uses a natural language inference pipeline to assess entailment.\n",
    "      - Text classification: uses a classification pipeline (as a proxy for relevance). \n",
    "    \"\"\"\n",
    "    \n",
    "    # normalize and compute token-level F1\n",
    "    def normalize(text: str) -> str:\n",
    "        return text.strip().lower()\n",
    "    \n",
    "    norm_gen = normalize(generated_answer)\n",
    "    norm_true = normalize(true_answer)\n",
    "    \n",
    "    # tokenize on whitespace.\n",
    "    gen_tokens = norm_gen.split()\n",
    "    true_tokens = norm_true.split()\n",
    "    common_tokens = set(gen_tokens) & set(true_tokens)\n",
    "    \n",
    "    if not common_tokens:\n",
    "        token_f1 = 0.0\n",
    "    else:\n",
    "        precision = len(common_tokens) / len(gen_tokens)\n",
    "        recall = len(common_tokens) / len(true_tokens)\n",
    "        token_f1 = 2 * (precision * recall) / (precision + recall)\n",
    "    \n",
    "    scores = {\"token_f1\": token_f1}\n",
    "    \n",
    "    # model names for the evaluation pipelines\n",
    "    model_names = [\n",
    "        \"sentence-transformers/all-MiniLM-L6-v2\", # embedding-based similarity.\n",
    "        \"cross-encoder/nli-roberta-base\", # zero-shot NLI.\n",
    "        \"distilbert/distilbert-base-uncased-finetuned-sst-2-english\" # text classification (relevance).\n",
    "    ]\n",
    "    \n",
    "    # embedding cosine_similarity\n",
    "    try:\n",
    "        goal = \"feature-extraction\"\n",
    "        # safe_generate_fit is expected to return embeddings for both texts. \n",
    "        embeddings = safe_generate_fit(generated_answer, true_answer, goal, model_names, max_retries=max_retries)\n",
    "        # mean-pool embeddings for each text.\n",
    "        pooled_gen = np.mean(np.array(embeddings[0]), axis=1).flatten()\n",
    "        pooled_true = np.mean(np.array(embeddings[1]), axis=1).flatten()\n",
    "        embedding_score = cosine_similarity([pooled_gen], [pooled_true])[0][0]\n",
    "        scores[\"embedding_similarity\"] = embedding_score\n",
    "    except Exception as e:\n",
    "        scores[\"embedding_similarity\"] = 0.0\n",
    "        print(f\"Embedding similarity evaluation failed: {e}\")\n",
    "    \n",
    "    # zero-shot NLI eval\n",
    "    try:\n",
    "        goal = \"zero-shot-classification\"\n",
    "        # compares generated_answer to the true_answer as candidate.\n",
    "        nli_result = safe_generate_fit(generated_answer, true_answer, goal, model_names, max_retries=max_retries)\n",
    "        # s/b a dictionary with a \"scores\" field.\n",
    "        nli_score = nli_result[\"scores\"][0]\n",
    "        scores[\"nli_score\"] = nli_score\n",
    "    except Exception as e:\n",
    "        scores[\"nli_score\"] = 0.0\n",
    "        print(f\"NLI evaluation failed: {e}\")\n",
    "    \n",
    "    # text classification eval\n",
    "    try:\n",
    "        goal = \"text-classification\"\n",
    "        # pipeline takes a concatenation of the texts.\n",
    "        text_class_result = safe_generate_fit(generated_answer, true_answer, goal, model_names, max_retries=max_retries)\n",
    "        text_class_score = text_class_result[0][\"score\"]\n",
    "        scores[\"text_classification_score\"] = text_class_score\n",
    "    except Exception as e:\n",
    "        scores[\"text_classification_score\"] = 0.0\n",
    "        print(f\"Text classification evaluation failed: {e}\")\n",
    "    \n",
    "    # aggregate threshold.\n",
    "    num_metrics_good = sum(1 for score in scores.values() if isinstance(score, float) and score > threshold)\n",
    "    scores[\"is_good_enough\"] = (num_metrics_good >= 2)\n",
    "    \n",
    "    return scores \n",
    "    \n",
    "def safe_generate_fit(response, keywords, goal, model_names, max_retries=5): \n",
    "    retries = 0\n",
    "    while retries < max_retries:\n",
    "        try:\n",
    "            if goal == \"feature-extraction\":\n",
    "                pipe = pipeline(goal, model=model_names[0])\n",
    "                result = pipe([response, keywords])\n",
    "                # clean up and return\n",
    "                del pipe\n",
    "                torch.cuda.empty_cache()\n",
    "                return result\n",
    "            \n",
    "            elif goal == \"zero-shot-classification\":\n",
    "                pipe = pipeline(goal, model=model_names[1])\n",
    "                # use true answer as a candidate label\n",
    "                result = pipe(response, candidate_labels=[keywords])\n",
    "                del pipe\n",
    "                torch.cuda.empty_cache()\n",
    "                return result\n",
    "            \n",
    "            elif goal == \"text-classification\":\n",
    "                pipe = pipeline(goal, model=model_names[2]) \n",
    "                result = pipe(response + \" \" + keywords)\n",
    "                del pipe\n",
    "                torch.cuda.empty_cache()\n",
    "                return result\n",
    "            \n",
    "        except torch.cuda.OutOfMemoryError:\n",
    "            print(f\"OutOfMemoryError. Attempt {retries + 1} of {max_retries}\")\n",
    "            # clean up GPU memory.\n",
    "            try:\n",
    "                del pipe\n",
    "            except Exception:\n",
    "                pass\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.reset_peak_memory_stats()\n",
    "            gc.collect()\n",
    "            time.sleep(5)\n",
    "            retries += 1\n",
    "    \n",
    "    raise RuntimeError(\"Maximum retries reached in safe_generate_fit.\")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "af2fb78f-10ff-4f0f-83d7-60244f6f3868",
   "metadata": {},
   "outputs": [],
   "source": [
    "#clear_cuda_memory()\n",
    "#system_prompt = prepare_system_prompt(docs20)\n",
    "#knowledge_cache, orig_cache_len = build_kv_cache(system_prompt)\n",
    "#print(\"Initial KV cache length:\", orig_cache_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c8b61ae5-f2a7-4de8-b308-2b34f7d16a50",
   "metadata": {},
   "outputs": [],
   "source": [
    "#docs20[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a54ae633-5712-4a9d-8bf8-dccb3caa1fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tests20[2]['What storage options are available in Boston for summer storage?']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1d7460cf-c774-466e-bb20-f2b3b959a0c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#question = 'What storage options are available in Boston for summer storage?'\n",
    "#generated_text = query_response(question, tokenizer, model, knowledge_cache, orig_cache_len)\n",
    "#generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6a99f673-7d95-4a4b-a47f-d0e9f75ad279",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_answer_against_full_context(generated_answer, full_documents, threshold=0.7):\n",
    "    full_context = \" \".join(full_documents)\n",
    "    \n",
    "    # cosine similarity  \n",
    "    embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "    answer_embedding = embedder.encode(generated_answer, convert_to_tensor=True)\n",
    "    context_embedding = embedder.encode(full_context, convert_to_tensor=True)\n",
    "    \n",
    "    cosine_sim = util.pytorch_cos_sim(answer_embedding, context_embedding).item() \n",
    "    \n",
    "    # BERTScore\n",
    "    try: \n",
    "        results =  score.compute(\n",
    "                    predictions=[generated_answer],\n",
    "                    references=[full_context],\n",
    "                    lang=\"en\",\n",
    "                    verbose=False\n",
    "                ) \n",
    "        bert_f1 = sum(results[\"f1\"]) / len(results[\"f1\"]) \n",
    "    except Exception as e:\n",
    "        print(\"BERTScore evaluation failed:\", e)\n",
    "        bert_f1 = 0.0\n",
    "\n",
    "    # If either metric meets or exceeds the threshold, consider the answer supported\n",
    "    return (cosine_sim >= threshold) or (bert_f1 >= threshold)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d39c36cb-c6f9-4add-b509-cbbca7f00cb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_allocated()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cf00ea8d-3172-46c5-a60d-d4899ada5ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_with_cleanup(model_name, quant_config, overall_i, n, max_retries=3):\n",
    "    retries = 0\n",
    "    model = None\n",
    "    tokenizer = None\n",
    "    while retries < max_retries:\n",
    "        try:\n",
    "            print(f\"Attempt {retries+1} to load the model...\")\n",
    "            tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "            model = AutoModelForCausalLM.from_pretrained(\n",
    "                model_name,\n",
    "                quantization_config=quant_config,\n",
    "                device_map='auto'\n",
    "            )\n",
    "            return model, tokenizer \n",
    "        except torch.cuda.OutOfMemoryError as e:\n",
    "            print(\"CUDA OutOfMemoryError caught during model loading:\", e)\n",
    "        except Exception as e:\n",
    "            print(\"Error during model loading:\", e)\n",
    "        # cleanup and prepare for the next attempt.\n",
    "        try:\n",
    "            del model, tokenizer, knowledge_cache, quant_config\n",
    "        except Exception:\n",
    "            pass\n",
    "        gb_value = torch.cuda.memory_allocated() / (1024**3)\n",
    "        print(f\"Memory Fail Before {retries} o-{overall_i},i-{n}: {gb_value:.2f} GB\") \n",
    "        clear_cuda_memory()\n",
    "        time.sleep(5)\n",
    "        gb_value = torch.cuda.memory_allocated() / (1024**3)\n",
    "        print(f\"Memory Fail After {retries} o-{overall_i},i-{n}: {gb_value:.2f} GB\") \n",
    "        retries += 1\n",
    "    raise RuntimeError(\"Failed to load the model after maximum retries.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8e770698-ad3d-48b2-a173-172af4e4decc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory o-1,i-5: 0.00 GB\n",
      "Attempt 1 to load the model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8e3596c6a69498e851e66e2fc7d6fa9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial KV cache length for n=5: 1347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory o-1,i-10: 2.75 GB\n",
      "Attempt 1 to load the model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db6ed83bfd8a4bfe8293a863a8486715",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial KV cache length for n=10: 2770\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory o-1,i-20: 2.75 GB\n",
      "Attempt 1 to load the model...\n",
      "Error during model loading: Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. \n",
      "Memory Fail Before 0 o-1,i-20: 2.75 GB\n",
      "Memory Fail After 0 o-1,i-20: 1.00 GB\n",
      "Attempt 2 to load the model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19dac2cc5fda447c9b72fbf71468c240",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial KV cache length for n=20: 4277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory o-2,i-5: 2.75 GB\n",
      "Attempt 1 to load the model...\n",
      "Error during model loading: Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. \n",
      "Memory Fail Before 0 o-2,i-5: 2.75 GB\n",
      "Memory Fail After 0 o-2,i-5: 1.00 GB\n",
      "Attempt 2 to load the model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c16585e9a8041d2beefe8bdc62fd82e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial KV cache length for n=5: 2530\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory o-2,i-10: 2.75 GB\n",
      "Attempt 1 to load the model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d587e39cfb748f4be8cc53602307231",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial KV cache length for n=10: 2071\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory o-2,i-20: 2.75 GB\n",
      "Attempt 1 to load the model...\n",
      "Error during model loading: Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. \n",
      "Memory Fail Before 0 o-2,i-20: 2.75 GB\n",
      "Memory Fail After 0 o-2,i-20: 1.00 GB\n",
      "Attempt 2 to load the model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf6ad12516fb419a9fb5b54c93fb68af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial KV cache length for n=20: 4676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory o-3,i-5: 2.75 GB\n",
      "Attempt 1 to load the model...\n",
      "Error during model loading: Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. \n",
      "Memory Fail Before 0 o-3,i-5: 2.75 GB\n",
      "Memory Fail After 0 o-3,i-5: 1.00 GB\n",
      "Attempt 2 to load the model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c82b9d97f54844f7a38bf352b0a0506e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial KV cache length for n=5: 1019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory o-3,i-10: 2.75 GB\n",
      "Attempt 1 to load the model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2084c49ffdea4e508cae2fbae9fa7c89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial KV cache length for n=10: 2482\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory o-3,i-20: 2.75 GB\n",
      "Attempt 1 to load the model...\n",
      "Error during model loading: Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. \n",
      "Memory Fail Before 0 o-3,i-20: 2.75 GB\n",
      "Memory Fail After 0 o-3,i-20: 1.00 GB\n",
      "Attempt 2 to load the model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba8e657be5cc4545b91aa6c57d69a260",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial KV cache length for n=20: 5534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory o-4,i-5: 2.75 GB\n",
      "Attempt 1 to load the model...\n",
      "Error during model loading: Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. \n",
      "Memory Fail Before 0 o-4,i-5: 2.75 GB\n",
      "Memory Fail After 0 o-4,i-5: 1.00 GB\n",
      "Attempt 2 to load the model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d2a572fd1c542f0ab8d4820a84c4db7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial KV cache length for n=5: 506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory o-4,i-10: 2.75 GB\n",
      "Attempt 1 to load the model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f90d1138e9d4d24bb9af549c5338a2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial KV cache length for n=10: 3809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory o-4,i-20: 2.75 GB\n",
      "Attempt 1 to load the model...\n",
      "Error during model loading: Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. \n",
      "Memory Fail Before 0 o-4,i-20: 2.75 GB\n",
      "Memory Fail After 0 o-4,i-20: 1.00 GB\n",
      "Attempt 2 to load the model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ba43a5700534209baf1505c4a530884",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial KV cache length for n=20: 4325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory o-5,i-5: 2.75 GB\n",
      "Attempt 1 to load the model...\n",
      "Error during model loading: Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. \n",
      "Memory Fail Before 0 o-5,i-5: 2.75 GB\n",
      "Memory Fail After 0 o-5,i-5: 1.00 GB\n",
      "Attempt 2 to load the model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ec055cf05ef456c8b84a45d56842a33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial KV cache length for n=5: 1104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory o-5,i-10: 2.75 GB\n",
      "Attempt 1 to load the model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ecb3a46652640fe93b56ff8ae6407a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial KV cache length for n=10: 1700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory o-5,i-20: 2.75 GB\n",
      "Attempt 1 to load the model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f35aedaf38e74e0b903ebf48e1329274",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial KV cache length for n=20: 5650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory o-6,i-5: 2.75 GB\n",
      "Attempt 1 to load the model...\n",
      "Error during model loading: Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. \n",
      "Memory Fail Before 0 o-6,i-5: 2.75 GB\n",
      "Memory Fail After 0 o-6,i-5: 1.00 GB\n",
      "Attempt 2 to load the model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53fcee1d023c443e8e84bd41afc08ae1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial KV cache length for n=5: 2843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory o-6,i-10: 2.75 GB\n",
      "Attempt 1 to load the model...\n",
      "Error during model loading: Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. \n",
      "Memory Fail Before 0 o-6,i-10: 2.75 GB\n",
      "Memory Fail After 0 o-6,i-10: 1.00 GB\n",
      "Attempt 2 to load the model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d25717aad787470184bf71e590932310",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial KV cache length for n=10: 1309\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory o-6,i-20: 2.75 GB\n",
      "Attempt 1 to load the model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76cd9f0dbe3b4ca3841db1f11aa7d17a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial KV cache length for n=20: 4007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory o-7,i-5: 2.75 GB\n",
      "Attempt 1 to load the model...\n",
      "Error during model loading: Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. \n",
      "Memory Fail Before 0 o-7,i-5: 2.75 GB\n",
      "Memory Fail After 0 o-7,i-5: 1.00 GB\n",
      "Attempt 2 to load the model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0f511b2835941fbaaa3528183f132af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial KV cache length for n=5: 1340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory o-7,i-10: 2.75 GB\n",
      "Attempt 1 to load the model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68ee663f90b44d2d974ff554ea056484",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial KV cache length for n=10: 2553\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory o-7,i-20: 2.75 GB\n",
      "Attempt 1 to load the model...\n",
      "Error during model loading: Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. \n",
      "Memory Fail Before 0 o-7,i-20: 2.75 GB\n",
      "Memory Fail After 0 o-7,i-20: 1.00 GB\n",
      "Attempt 2 to load the model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11e5336270a04d71b6f1dc2557e133b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial KV cache length for n=20: 4713\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory o-8,i-5: 2.75 GB\n",
      "Attempt 1 to load the model...\n",
      "Error during model loading: Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. \n",
      "Memory Fail Before 0 o-8,i-5: 2.75 GB\n",
      "Memory Fail After 0 o-8,i-5: 1.00 GB\n",
      "Attempt 2 to load the model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a58acd18d5c141e9be79df1d24c4bfd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial KV cache length for n=5: 1333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory o-8,i-10: 2.75 GB\n",
      "Attempt 1 to load the model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29e25cc927814dd5ab1cbfe4b62c566a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial KV cache length for n=10: 2139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory o-8,i-20: 2.75 GB\n",
      "Attempt 1 to load the model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0b65f90a276416294fbaa23c4c39df5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial KV cache length for n=20: 4838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory o-9,i-5: 2.75 GB\n",
      "Attempt 1 to load the model...\n",
      "Error during model loading: Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. \n",
      "Memory Fail Before 0 o-9,i-5: 2.75 GB\n",
      "Memory Fail After 0 o-9,i-5: 1.00 GB\n",
      "Attempt 2 to load the model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d43b9a11e11542109c6ace570eef57f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial KV cache length for n=5: 7791\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory o-9,i-10: 2.75 GB\n",
      "Attempt 1 to load the model...\n",
      "Error during model loading: Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. \n",
      "Memory Fail Before 0 o-9,i-10: 2.75 GB\n",
      "Memory Fail After 0 o-9,i-10: 1.00 GB\n",
      "Attempt 2 to load the model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1ecf98971bf4372be8e88409a3254ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial KV cache length for n=10: 1235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory o-9,i-20: 2.75 GB\n",
      "Attempt 1 to load the model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d576fdda7d1548ae8f6b4c74d885e7ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial KV cache length for n=20: 4943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory o-10,i-5: 2.75 GB\n",
      "Attempt 1 to load the model...\n",
      "Error during model loading: Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. \n",
      "Memory Fail Before 0 o-10,i-5: 2.75 GB\n",
      "Memory Fail After 0 o-10,i-5: 1.00 GB\n",
      "Attempt 2 to load the model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12d001e293b0400791a5ee7a4d6e4911",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial KV cache length for n=5: 736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory o-10,i-10: 2.75 GB\n",
      "Attempt 1 to load the model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66b0394f841d419b9719613e3b8b7c95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial KV cache length for n=10: 3851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory o-10,i-20: 2.75 GB\n",
      "Attempt 1 to load the model...\n",
      "Error during model loading: Some modules are dispatched on the CPU or the disk. Make sure you have enough GPU RAM to fit the quantized model. If you want to dispatch the model on the CPU or the disk while keeping these modules in 32-bit, you need to set `llm_int8_enable_fp32_cpu_offload=True` and pass a custom `device_map` to `from_pretrained`. Check https://huggingface.co/docs/transformers/main/en/main_classes/quantization#offload-between-cpu-and-gpu for more details. \n",
      "Memory Fail Before 0 o-10,i-20: 2.75 GB\n",
      "Memory Fail After 0 o-10,i-20: 1.00 GB\n",
      "Attempt 2 to load the model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7bd5fc6e9c94cae98ef1ec038546b0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial KV cache length for n=20: 3350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "# iterave test 10x 5, 10, and 20 docs\n",
    "for overall_i in range(1, 11):\n",
    "    for n in [5,10,20]: #5,10,20\n",
    "        # generate samples\n",
    "        docs_n, tests_n = select_random_n(all_test_data,n)\n",
    "        #clear mem\n",
    "        clear_cuda_memory()\n",
    "        try:\n",
    "            del model, tokenizer, knowledge_cache, quant_config\n",
    "        except Exception:\n",
    "            pass\n",
    "        time.sleep(1)\n",
    "        gb_value = torch.cuda.memory_allocated() / (1024**3)\n",
    "        print(f\"Memory o-{overall_i},i-{n}: {gb_value:.2f} GB\") \n",
    "\n",
    "        quant_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16\n",
    "        )\n",
    "        model_name = \"microsoft/Phi-3.5-mini-instruct\"\n",
    "        \n",
    "        # attempt to load the model up to 3 times,\n",
    "        # cleaning GPU memory if an error occurs.\n",
    "        model, tokenizer = load_model_with_cleanup(model_name, quant_config, overall_i, n)\n",
    "    \n",
    "        #prepare model\n",
    "        system_prompt = prepare_system_prompt(docs_n)\n",
    "        knowledge_cache, orig_cache_len = build_kv_cache(system_prompt)\n",
    "        print(f\"Initial KV cache length for n={n}:\", orig_cache_len)\n",
    "    \n",
    "        n_results = []\n",
    "        n_pass = []\n",
    "        for idx, doc in enumerate(docs_n):\n",
    "            rand_question = random.sample(sorted(tests_n[idx]), 1)[0]\n",
    "            rand_answer = tests_n[idx][rand_question]\n",
    "            generated_answer = query_response(rand_question, tokenizer, model, knowledge_cache, orig_cache_len)\n",
    "    \n",
    "            this_results = evaluate_generated_answer(generated_answer, rand_answer)\n",
    "            validate_overall = this_results['is_good_enough'] if this_results['is_good_enough'] else check_answer_against_full_context(generated_answer, docs_n, threshold=0.7)\n",
    "            n_results.append({\n",
    "                'doc':doc,\n",
    "                'query':rand_question,\n",
    "                'answer_true':rand_answer,\n",
    "                'answer_gen':generated_answer,\n",
    "                'results':this_results,\n",
    "                'is_good_enough': this_results['is_good_enough'],\n",
    "                'validate_overall_doc': validate_overall\n",
    "            }\n",
    "            )\n",
    "            n_pass.append(validate_overall)\n",
    "        \n",
    "        # save results\n",
    "        file_name = f\"iteration_results_n_{n}.json\"\n",
    "        overall_file_name = f\"iteration_passfail_n_{n}.json\" \n",
    "        with open(file_name, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump({f\"{n}_all_result2\": n_results}, f, indent=4)\n",
    "        with open(overall_file_name, \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump({f\"{n}_all_passfail2\": n_pass}, f, indent=4)\n",
    "            \n",
    "        label1 = str(n)+\"_\"+str(overall_i)+'_all_result_r3'\n",
    "        label2 = str(n)+\"_\"+str(overall_i)+'_all_passfail_r3'\n",
    "        results.append({label1: n_results,\n",
    "                        label2: n_pass})\n",
    "with open(\"overall_results_r3.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2755a08-bbd3-49d1-86fd-c0f15b876a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_n[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54eaf7ab-fda8-4052-afec-17461f192911",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_results[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c73674-280a-494e-bee6-4d755355a215",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_results[2]['answer_gen']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
